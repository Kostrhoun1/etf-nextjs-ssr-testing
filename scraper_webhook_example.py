# PÅ™Ã­klad kÃ³du pro pÅ™idÃ¡nÃ­ na konec final_scraper.py

import requests
import logging
import os

def trigger_website_refresh():
    """
    ZavolÃ¡ webhook pro aktualizaci homepage po dokonÄenÃ­ scrapingu
    """
    try:
        # URL webhook endpointu
        webhook_url = "https://etfpruvodce.cz/api/revalidate"
        
        # Secret key pro zabezpeÄenÃ­
        secret_key = os.getenv('WEBSITE_REFRESH_SECRET', 'etf_refresh_2025')
        
        # Payload
        payload = {
            "secret": secret_key,
            "source": "scraper",
            "timestamp": datetime.now().isoformat()
        }
        
        print("ğŸ”„ Triggering website refresh...")
        
        # Zavolej webhook
        response = requests.post(
            webhook_url, 
            json=payload,
            timeout=30,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… Website refresh successful: {result.get('message')}")
            print(f"ğŸ“… Timestamp: {result.get('timestamp')}")
        else:
            print(f"âŒ Website refresh failed: {response.status_code}")
            print(f"Response: {response.text}")
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ Network error during website refresh: {e}")
    except Exception as e:
        print(f"âŒ Unexpected error during website refresh: {e}")

# PÅ˜IDAT NA KONEC final_scraper.py:
"""
def main():
    # ... existujÃ­cÃ­ scraper logika ...
    
    # Po ÃºspÄ›Å¡nÃ©m uploadu do Supabase:
    print("âœ… All ETF data successfully uploaded to Supabase")
    
    # Zavolej refresh homepage
    trigger_website_refresh()
    
    print("ğŸ‰ Scraping process completed!")

if __name__ == "__main__":
    main()
"""

# Environment variable pro secret:
# export WEBSITE_REFRESH_SECRET="etf_refresh_2025"